{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshnakAGQ/DistractedDriver/blob/master/CS175Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwqRCOOqopRu",
        "colab_type": "text"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWv3NeCcpRKK",
        "colab_type": "text"
      },
      "source": [
        "##Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIxrEcZhFGvf",
        "colab_type": "code",
        "outputId": "fd516452-3274-4a43-8466-8a785b7d8770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "!pip install tf-nightly\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/29/8c0a64eeb01ad463d58717914deb18596e8d5ebed560ea841ffebc981dd5/tf_nightly-2.3.0.dev20200610-cp36-cp36m-manylinux2010_x86_64.whl (349.8MB)\n",
            "\u001b[K     |████████████████████████████████| 349.8MB 46kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.5)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.2.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.34.2)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/47/f322282f1a9e6286f00d10d8b35e6924e7ea4ac3c1410cfcfcce02d1746b/tf_estimator_nightly-2.3.0.dev2020060901-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.0)\n",
            "Collecting tb-nightly<2.4.0a0,>=2.3.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/dd/9717f4bcf6ab9c043794815772940c2c000382adf26978014fb5db93ebf5/tb_nightly-2.3.0a20200610-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 51.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.4.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.29.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly) (47.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.0.post3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.8)\n",
            "Installing collected packages: tf-estimator-nightly, tb-nightly, tf-nightly\n",
            "Successfully installed tb-nightly-2.3.0a20200610 tf-estimator-nightly-2.3.0.dev2020060901 tf-nightly-2.3.0.dev20200610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKRq_ruELmkW",
        "colab_type": "code",
        "outputId": "171348eb-41d2-4f23-9c35-dfd90fa895e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0-dev20200610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zKAvv4JpaYd",
        "colab_type": "text"
      },
      "source": [
        "##Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGfVWT48pCNv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "8285368d-f19a-4560-9d19-ea75cef59520"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT2hXoZDpp4C",
        "colab_type": "text"
      },
      "source": [
        "##Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHYGZPgIo6rE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "87a467d2-aeef-4b18-cb56-a9bc8c97b0a0"
      },
      "source": [
        "!pip install kaggle\n",
        "import os # Sets the environment variables in the root folder \n",
        "os.environ['KAGGLE_USERNAME'] = \"unorna\"\n",
        "os.environ['KAGGLE_KEY'] = \"0a21e3dad74e374d7054ac533a76ede3\"\n",
        "import kaggle\n",
        "kaggle.api.authenticate()\n",
        "kaggle.api.competition_download_files('state-farm-distracted-driver-detection', path = 'data/DistractedDriver')\n",
        "!unzip -q data/DistractedDriver/state-farm-distracted-driver-detection.zip  #unzips kaggle data into content/data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.4.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0tsPMK_IQnt",
        "colab_type": "text"
      },
      "source": [
        "#Import image data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAnrkDPcFQLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size = (224, 224) #image size for image net is 224 x 224 x 3\n",
        "batch_size = 32\n",
        "\n",
        "full_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"imgs/train\",\n",
        "    image_size = image_size,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False\n",
        ")\n",
        "\n",
        "full_ds_s = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"imgs/train\",\n",
        "    seed = 1337,\n",
        "    image_size = image_size,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"imgs/train\",\n",
        "    seed = 1337,\n",
        "    image_size = image_size,\n",
        "    batch_size = batch_size,\n",
        "    subset = 'training',\n",
        "    validation_split = 0.2,\n",
        "    shuffle = True\n",
        ").prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"imgs/train\",\n",
        "    seed = 1337,\n",
        "    image_size = image_size,\n",
        "    batch_size = batch_size,\n",
        "    subset = 'validation',\n",
        "    validation_split = 0.2,\n",
        "    shuffle = True\n",
        ").prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qnhgVpAoa59",
        "colab_type": "text"
      },
      "source": [
        "##View Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrV8Che3Nc5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in full_ds_s.take(1):\n",
        "    for i in range(16):\n",
        "        ax = plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRbF5vPBojQK",
        "colab_type": "text"
      },
      "source": [
        "#Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WiXL3Aw7UMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate(dataset, n_splits, batch_size=32, seed=0):\n",
        "\n",
        "  \"\"\"\n",
        "  Given a dataset, splits it into {n_splits} subsets, where for \n",
        "  any given index, train_ds[i] contains all the subsets except \n",
        "  the portion in val_ds[i]\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataset : tensorflow.data.dataset\n",
        "      Full dataset to split\n",
        "    \n",
        "  n_splits : int\n",
        "      Number of splits; XFold # (at least 2 or greater)\n",
        "      \n",
        "  Returns\n",
        "  -------\n",
        "  train_ds : list (dtype=ConcatenateDataset)\n",
        "      List of datasets not containing the ith portion of data\n",
        "      \n",
        "  val_ds : list (dtype=ShardDataset)\n",
        "      List of datasets containing the ith portion of data\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize lists\n",
        "  val_ds = []\n",
        "  train_ds = []\n",
        "\n",
        "  # Create val_ds first\n",
        "  for i in range(n_splits):\n",
        "    val_ds.append(dataset.shard(num_shards=n_splits, index=i).shuffle(buffer_size=batch_size * 8, seed=seed))\n",
        "\n",
        "  # Create train_ds using val_ds indexes other than the current index\n",
        "  for i in range(n_splits):\n",
        "    if i == 0:\n",
        "      train_ds.append(val_ds[1])\n",
        "    else:\n",
        "      train_ds.append(val_ds[0])\n",
        "    for a in range(1, i):\n",
        "      train_ds[i] = train_ds[i].concatenate(val_ds[a].shuffle(buffer_size=batch_size * 8, seed=seed))\n",
        "    for b in range(i+1, n_splits):\n",
        "      if not(i==0 and b==1):\n",
        "        train_ds[i] = train_ds[i].concatenate(val_ds[b].shuffle(buffer_size=batch_size * 8, seed=seed))\n",
        "\n",
        "  return train_ds, val_ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vkN2721d3qH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_splits = 5\n",
        "\n",
        "cv_training_ds, cv_val_ds = crossvalidate(full_ds, n_splits, seed=1337)\n",
        "\n",
        "\"\"\"# Set up prefetching\n",
        "for i in range(len(cv_training_ds)):m\n",
        "  cv_training_ds[i] = cv_training_ds[i].prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  cv_val_ds[i] = cv_val_ds[i].prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\"\"\"\n",
        "print(len(cv_training_ds), cv_training_ds[0])\n",
        "print(len(cv_val_ds), cv_val_ds[0])\n",
        "\n",
        "print(tf.data.experimental.cardinality(full_ds))\n",
        "print(tf.data.experimental.cardinality(cv_training_ds[1]))\n",
        "print(tf.data.experimental.cardinality(train_ds))\n",
        "print(tf.data.experimental.cardinality(cv_val_ds[1]))\n",
        "print(tf.data.experimental.cardinality(validation_ds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KROQBQIGF8Ks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUpPKntj4_yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"import time\n",
        "def benchmark(dataset, num_epochs=2):\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    model = VGG16(include_top=False,weights='imagenet', input_shape=(320,240,3))\n",
        "    flatten = Flatten()(model.outputs[-1])\n",
        "    output = Dense(10, activation = 'softmax')(flatten)\n",
        "    model = Model(inputs=model.inputs,outputs=output)\n",
        "\n",
        "    # Might have to copy data from Google Drive to the Google Environment\n",
        "    model.compile(optimizer='adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
        "    model.fit(dataset, epochs=10)\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores = model.evaluate(dataset, verbose=0)\n",
        "    \n",
        "    tf.print(\"Execution time:\", time.perf_counter() - start_time)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llk69TQ7YMju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "for images, labels in full_ds_s.take(1):\n",
        "  print(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaim5xut_lB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch size = 32\n",
        "#benchmark(cv_val_ds[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVgL2BBUMwLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch size = 32\n",
        "#benchmark(cv_val_ds[0].prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcoczWzVlQJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch size = 128\n",
        "#benchmark(cv_val_ds[0].prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O_2VNOEqrP5",
        "colab_type": "text"
      },
      "source": [
        "#Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjp5I8cerM5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode ='min', patience=4),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5', save_best_only=True)\n",
        "]\n",
        "inception_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode ='min', patience=4),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath='inceptionmodel.{epoch:02d}-{val_loss:.2f}.h5', save_best_only=True)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi-JKjmMq2XR",
        "colab_type": "text"
      },
      "source": [
        "##VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqzq4ITIsLu0",
        "colab_type": "text"
      },
      "source": [
        "###Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqXo7wj5C-7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = VGG16(include_top=True, weights = None, classes = 10, input_shape=(224, 224, 3))\n",
        "#model = VGG16(include_top=False,weights='imagenet', input_shape=(224,224,3))\n",
        "#flatten = Flatten()(model.outputs[-1])\n",
        "#output = Dense(10, activation = 'softmax')(flatten)\n",
        "#model = Model(inputs=model.inputs,outputs=output)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RK6lvwVJkgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.save_weights('drive/My Drive/my_model_weights.h5') # Save the current model into Google Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQZj-LDErqzx",
        "colab_type": "text"
      },
      "source": [
        "###Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMZDCqvaQSd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def VGG_preprocess(image,score):\n",
        "  image = keras.applications.vgg16.preprocess_input(image)\n",
        "  if score is None:\n",
        "        return image\n",
        "  else:\n",
        "        return image, score\n",
        "\n",
        "accuracies = []\n",
        "losses = []\n",
        "\n",
        "for i in range(len(cv_training_ds)):\n",
        "\n",
        "  # Pre-processing\n",
        "  train = cv_training_ds[i].map(VGG_preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  train = train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  val = cv_val_ds[i].map(VGG_preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  val = val.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  model = VGG16(include_top=True, weights = None, classes = 10, input_shape=(224, 224, 3))\n",
        "  #model = VGG16(include_top=False, classes = 10, input_shape=(320,240,3))\n",
        "  #flatten = Flatten()(model.outputs[-1])\n",
        "  #output = Dense(10, activation = 'softmax')(flatten)\n",
        "  #model = Model(inputs=model.inputs,outputs=output)\n",
        "\n",
        "  # Might have to copy data from Google Drive to the Google Environment\n",
        "  model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train, epochs=10, validation_data=val, callbacks=my_callbacks)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(val, verbose=0)\n",
        "  accuracies.append(scores[1] * 100)\n",
        "  losses.append(scores[0])\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(accuracies)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {losses[i]} - Accuracy: {accuracies[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(accuracies)} (+- {np.std(accuracies)})')\n",
        "print(f'> Loss: {np.mean(losses)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCnuydP9q-z_",
        "colab_type": "text"
      },
      "source": [
        "##Resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0d_NtmWsPlu",
        "colab_type": "text"
      },
      "source": [
        "###Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4TM1M4bfQGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet_model = ResNet50(include_top =False,weights='imagenet', input_shape=(320,240,3))\n",
        "resnet_flatten = Flatten()(resnet_model.outputs[-1])\n",
        "resnet_output = Dense(10, activation = 'softmax')(resnet_flatten)\n",
        "resnet_model = Model(inputs=resnet_model.inputs,outputs=resnet_output)\n",
        "resnet_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qeXAeooryXI",
        "colab_type": "text"
      },
      "source": [
        "###Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHcsWon8jP4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracies = []\n",
        "losses = []\n",
        "\n",
        "for i in range(len(cv_training_ds)):\n",
        "  resnet_model = ResNet50(include_top =False,weights='imagenet', input_shape=(320,240,3))\n",
        "  resnet_flatten = Flatten()(resnet_model.outputs[-1])\n",
        "  resnet_output = Dense(10, activation = 'softmax')(resnet_flatten)\n",
        "  resnet_model = Model(inputs=resnet_model.inputs,outputs=resnet_output)\n",
        "\n",
        "  resnet_model.compile(optimizer='nadam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
        "  resnet_model.fit(cv_training_ds[i], epochs=10, validation_data=cv_val_ds[i], callbacks=my_callbacks)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = resnet_model.evaluate(cv_val_ds[i], verbose=0)\n",
        "  accuracies.append(scores[1] * 100)\n",
        "  losses.append(scores[0])\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(accuracies)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {losses[i]} - Accuracy: {accuracies[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(accuracies)} (+- {np.std(accuracies)})')\n",
        "print(f'> Loss: {np.mean(losses)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQEb8EgIrB1j",
        "colab_type": "text"
      },
      "source": [
        "##Inception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMPLA6N2sQYE",
        "colab_type": "text"
      },
      "source": [
        "###Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEGrvURJgGI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inception_model = InceptionV3(include_top =False,weights='imagenet', input_shape=(320,240,3))\n",
        "inception_flatten = Flatten()(inception_model.outputs[-1])\n",
        "inception_output = Dense(10, activation = 'softmax')(inception_flatten)\n",
        "inception_model = Model(inputs=inception_model.inputs,outputs=inception_output)\n",
        "inception_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skbwx73dr-64",
        "colab_type": "text"
      },
      "source": [
        "###Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny7NqD2njVca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracies = []\n",
        "losses = []\n",
        "\n",
        "for i in range(len(cv_training_ds)):\n",
        "  inception_model = InceptionV3(include_top =False,weights='imagenet', input_shape=(320,240,3))\n",
        "  inception_flatten = Flatten()(inception_model.outputs[-1])\n",
        "  inception_output = Dense(10, activation = 'softmax')(inception_flatten)\n",
        "  inception_model = Model(inputs=inception_model.inputs,outputs=inception_output)\n",
        "\n",
        "  inception_model.compile(optimizer='nadam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
        "  inception_model.fit(cv_training_ds[i], epochs=10, validation_data=cv_val_ds[i], callbacks=inception_callbacks)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = inception_model.evaluate(cv_val_ds[i], verbose=0)\n",
        "  accuracies.append(scores[1] * 100)\n",
        "  losses.append(scores[0])\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(accuracies)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {losses[i]} - Accuracy: {accuracies[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(accuracies)} (+- {np.std(accuracies)})')\n",
        "print(f'> Loss: {np.mean(losses)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EINoXcaeDUMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#inception_model.save(\"drive/My Drive/inceptionv3_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL4IpabgsfAS",
        "colab_type": "text"
      },
      "source": [
        "#Other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZlezWHUs_jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# it = iter(full_ds) # Dataset iterable\n",
        "# images = next(it)[0].numpy() # Get first batch of images\n",
        "# images = preprocess_input(images) # Preprocess images. Not sure if this necessary or not\n",
        "\n",
        "# yhat = model.predict(images)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}